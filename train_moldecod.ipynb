{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f20e2ec-2859-498c-8ac8-7cc86f09bcb0",
   "metadata": {},
   "source": [
    "# Train MolDecod - Decoder-only transformer with rotary positional encoding for molecule generation\n",
    "\n",
    "This is the end-to-end notebook to train and evaluate MolDecod, a small decoder-only transformer for generating molecules represented as SMILES, and its tokenizer (using SentencePiece).\n",
    "\n",
    "The entire notebook takes ~5 hours to run on an NVIDIA A100.\n",
    "\n",
    "### Data\n",
    "\n",
    "We train the model on a combination of two datasets for molecule generation (cf [here](https://tdcommons.ai/generation_tasks/molgen/)):\n",
    "- MOSES, a benchmark platform for distribution learning based molecule generation, processed from the ZINC Clean Leads dataset, containing ~ 2 million molecules.\n",
    "- ChEMBL, a manually curated database of bioactive molecules with drug-like properties, containing ~ 2 million molecules.\n",
    "\n",
    "The split results in 2.7 million molecules in the training set, and ~600k molecules in both the validation and test sets.\n",
    "\n",
    "### Result\n",
    "\n",
    "The final model has 5 million parameters and is very performant for its size.\n",
    "\n",
    "On 10,000 generated molecules:\n",
    "- Validity: 0.95\n",
    "- Uniqueness: 0.95\n",
    "- Diversity: 0.87\n",
    "- Novelty: 0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f204f-060f-4611-8927-1eaee72ed204",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80311e-a39c-477e-bafe-68c26448d1f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T11:25:46.375496Z",
     "iopub.status.busy": "2024-08-07T11:25:46.375027Z",
     "iopub.status.idle": "2024-08-07T11:26:17.614936Z",
     "shell.execute_reply": "2024-08-07T11:26:17.614426Z",
     "shell.execute_reply.started": "2024-08-07T11:25:46.375476Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install torch scikit-learn PyTDC rdkit-pypi sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73fe0120-dd08-41e5-ad89-710bbf95201c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T11:26:17.616356Z",
     "iopub.status.busy": "2024-08-07T11:26:17.616181Z",
     "iopub.status.idle": "2024-08-07T11:27:27.450455Z",
     "shell.execute_reply": "2024-08-07T11:27:27.449881Z",
     "shell.execute_reply.started": "2024-08-07T11:26:17.616338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n",
      "Number of GPUs available: 1\n",
      "Device 1: NVIDIA A100-SXM4-80GB\n",
      "  - Total Memory: 79.32 GB\n",
      "  - Multiprocessors: 108\n",
      "  - Compute Capability: 8.0\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Check if GPU is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if cuda_available:\n",
    "    print(\"CUDA is available.\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i+1}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  - Total Memory: {torch.cuda.get_device_properties(i).total_memory / (1024 ** 3):.2f} GB\")\n",
    "        print(f\"  - Multiprocessors: {torch.cuda.get_device_properties(i).multi_processor_count}\")\n",
    "        print(f\"  - Compute Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# Use GPU if available, CPU if not\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Pre-define seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if cuda_available:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886d6330-6156-48d5-ac0e-9eb65ad81e60",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We import, concatenate and re-split MOSES and ChEMBL, because the original validation set was too small and led to overfitting.\n",
    "\n",
    "You can also specify a path, where to save the data by specifiying:\n",
    "- `path = ''`\n",
    "- `data_source = MolGen(name=source_name, path=path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19174fba-577e-48d4-a84d-e9c8b9e17dd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T11:27:27.451407Z",
     "iopub.status.busy": "2024-08-07T11:27:27.451142Z",
     "iopub.status.idle": "2024-08-07T11:27:36.169421Z",
     "shell.execute_reply": "2024-08-07T11:27:36.169036Z",
     "shell.execute_reply.started": "2024-08-07T11:27:27.451390Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   smiles\n",
      "0           COCCOCC1NCCc2cc3ccc(O)cc3cc21\n",
      "1     Cc1ccc(NC(=O)N2CCCC2C(O)C(F)(F)F)s1\n",
      "2  COc1ccc(C(C)NC(=O)CCn2cc(C)cn2)c(OC)c1\n",
      "3   COc1ccc(Cl)cc1-c1ccc(C#N)c(=O)n1C(C)C\n",
      "4         O=C(c1cc(-n2cccn2)ccn1)N1CCOCC1\n",
      "Train set shape: (2714758, 1)\n",
      "Validation set shape: (581853, 1)\n",
      "Test set shape: (581756, 1)\n"
     ]
    }
   ],
   "source": [
    "from tdc.generation import MolGen\n",
    "\n",
    "# Get MolGen datasets, and split 70/15/15 (train/validation/test)\n",
    "def load_and_prepare_data(source_name, test_size=0.15, val_size=0.1765, random_state=42):\n",
    "    # Initialize and get data splits\n",
    "    data_source = MolGen(name=source_name)\n",
    "    split_data = data_source.get_split()\n",
    "\n",
    "    # Combine train, valid, and test into a single DataFrame and drop NaNs\n",
    "    df = pd.concat([split_data['train'], split_data['valid'], split_data['test']], ignore_index=True)\n",
    "    df.dropna(subset=['smiles'], inplace=True)\n",
    "\n",
    "    # Train-validation-test split\n",
    "    train_val, test = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "    train, valid = train_test_split(train_val, test_size=val_size, random_state=random_state)\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "# Load and prepare datasets\n",
    "train_moses, valid_moses, test_moses = load_and_prepare_data('MOSES')\n",
    "train_chembl, valid_chembl, test_chembl = load_and_prepare_data('ChEMBL')\n",
    "\n",
    "# Concatenate the two sources\n",
    "train_df = pd.concat([train_moses, train_chembl], ignore_index=True)\n",
    "valid_df = pd.concat([valid_moses, valid_chembl], ignore_index=True)\n",
    "test_df = pd.concat([test_moses, test_chembl], ignore_index=True)\n",
    "\n",
    "# Display the first few entries and shapes of datasets\n",
    "print(train_df.head())\n",
    "print(f\"Train set shape: {train_df.shape}\")\n",
    "print(f\"Validation set shape: {valid_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1b0aff-6374-4530-9da1-423820053741",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "We train a SentencePiece tokenizer, with a vocab size of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "babb655c-87ee-43d2-a582-5df9b4f93f95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T11:27:36.170739Z",
     "iopub.status.busy": "2024-08-07T11:27:36.170558Z",
     "iopub.status.idle": "2024-08-07T11:27:36.174864Z",
     "shell.execute_reply": "2024-08-07T11:27:36.174475Z",
     "shell.execute_reply.started": "2024-08-07T11:27:36.170692Z"
    }
   },
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "SOS_TOKEN = '<SOS>'  # start of sequence\n",
    "EOS_TOKEN = '<EOS>'  # end of sequence\n",
    "PAD_TOKEN = '<PAD>'  # padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6654098f-f9b9-4cce-8741-6b76fc4af06b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T21:35:00.625231Z",
     "iopub.status.busy": "2024-08-06T21:35:00.624762Z",
     "iopub.status.idle": "2024-08-06T22:00:37.906438Z",
     "shell.execute_reply": "2024-08-06T22:00:37.905959Z",
     "shell.execute_reply.started": "2024-08-06T21:35:00.625213Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:42:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:42:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:42:36] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:43:31] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:43:31] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:43:31] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:44:25] WARNING: not removing hydrogen atom without neighbors\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: train_smiles.txt\n",
      "  input_format: \n",
      "  model_prefix: smiles\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <EOS>\n",
      "  user_defined_symbols: <PAD>\n",
      "  user_defined_symbols: <SOS>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: train_smiles.txt\n",
      "trainer_interface.cc(145) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(145) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(122) LOG(WARNING) Too many sentences are loaded! (2714758), which may slow down training.\n",
      "trainer_interface.cc(124) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(127) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 2714758 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <EOS>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <PAD>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <SOS>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=129096807\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9582% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=33\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999582\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 2714758 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=93372456\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 5669 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 2714758\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 2681082\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 2681082 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3083 obj=124.421 num_tokens=81447486 num_tokens/piece=26418.3\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2283 obj=112.114 num_tokens=85215127 num_tokens/piece=37325.9\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1151 obj=112.619 num_tokens=88178349 num_tokens/piece=76610.2\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1139 obj=111.011 num_tokens=88563777 num_tokens/piece=77755.7\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1053 obj=111.209 num_tokens=89123274 num_tokens/piece=84637.5\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1050 obj=111.001 num_tokens=90002138 num_tokens/piece=85716.3\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: smiles.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: smiles.vocab\n",
      "[21:53:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:54:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:54:14] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:55:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:55:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:55:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:56:11] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "# Prepare SMILES data for SentencePiece tokenizer\n",
    "def prepare_smiles_file(dataframe, file_name='train_smiles.txt'):\n",
    "    smiles_strings = dataframe['smiles'].tolist()\n",
    "    with open(file_name, 'w') as f:\n",
    "        for smile in smiles_strings:\n",
    "            mol = Chem.MolFromSmiles(smile)\n",
    "            if mol is not None:\n",
    "                canonical_smile = Chem.MolToSmiles(mol, canonical=True)\n",
    "                f.write(canonical_smile + '\\n')  # Write each SMILES on a new line\n",
    "            else:\n",
    "                print(f\"Invalid SMILES string: {smile}\")\n",
    "\n",
    "# Prepare training data (creates 'train_smiles.txt')\n",
    "prepare_smiles_file(train_df, 'train_smiles.txt')\n",
    "\n",
    "# Train and save SentencePiece tokenizer model\n",
    "spm.SentencePieceTrainer.Train(input='train_smiles.txt', model_prefix='smiles', vocab_size=1000, user_defined_symbols=[EOS_TOKEN, PAD_TOKEN, SOS_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f5d6f1-a927-438f-a008-c8100b26046e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T11:27:36.175541Z",
     "iopub.status.busy": "2024-08-07T11:27:36.175390Z",
     "iopub.status.idle": "2024-08-07T11:40:48.277153Z",
     "shell.execute_reply": "2024-08-07T11:40:48.276583Z",
     "shell.execute_reply.started": "2024-08-07T11:27:36.175538Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:34:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:34:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:35:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:35:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:35:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:35:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:36:48] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "# Load SentencePiece tokenizer model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('smiles.model')\n",
    "\n",
    "# Dataset Class\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, dataframe, sp_model):\n",
    "        self.smiles = dataframe['smiles'].tolist()\n",
    "        self.sp = sp_model\n",
    "\n",
    "        # Process SMILES using the tokenizer\n",
    "        self.processed_smiles = []\n",
    "        for smile in self.smiles:\n",
    "            mol = Chem.MolFromSmiles(smile)\n",
    "            if mol is not None:  # Ensure valid SMILES\n",
    "                canonical_smile = Chem.MolToSmiles(mol, canonical=True)\n",
    "                self.processed_smiles.append(canonical_smile)\n",
    "            else:\n",
    "                print(f\"Invalid SMILES string: {smile}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smile = self.processed_smiles[idx]\n",
    "        # Tokenize\n",
    "        tokens = [self.sp.piece_to_id(SOS_TOKEN)] + self.sp.encode(smile) + [self.sp.piece_to_id(EOS_TOKEN)]\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "# Custom collate function for padding sequences\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=sp.piece_to_id(PAD_TOKEN))\n",
    "    return batch\n",
    "\n",
    "# Dataset setup\n",
    "train_dataset = SMILESDataset(train_df, sp)\n",
    "valid_dataset = SMILESDataset(valid_df, sp)\n",
    "test_dataset = SMILESDataset(test_df, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55fc3c31-dff8-4777-af93-3da8bebbe2a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T09:46:56.976542Z",
     "iopub.status.busy": "2024-08-07T09:46:56.975885Z",
     "iopub.status.idle": "2024-08-07T09:48:13.345365Z",
     "shell.execute_reply": "2024-08-07T09:48:13.344178Z",
     "shell.execute_reply.started": "2024-08-07T09:46:56.976542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in training dataset: 96855925\n",
      "Total number of tokens in validation dataset: 20757282\n",
      "Total number of tokens in test dataset: 20727137\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of tokens in the dataset\n",
    "def calculate_total_tokens(dataset):\n",
    "    total_tokens = 0\n",
    "    for i in range(len(dataset)):\n",
    "        tokens = dataset[i]\n",
    "        total_tokens += len(tokens)\n",
    "    return total_tokens\n",
    "\n",
    "total_train_tokens = calculate_total_tokens(train_dataset)\n",
    "total_valid_tokens = calculate_total_tokens(valid_dataset)\n",
    "total_test_tokens = calculate_total_tokens(test_dataset)\n",
    "\n",
    "print(f\"Total number of tokens in training dataset: {total_train_tokens}\")\n",
    "print(f\"Total number of tokens in validation dataset: {total_valid_tokens}\")\n",
    "print(f\"Total number of tokens in test dataset: {total_test_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22259afb-9dd2-4544-bbb2-c1a47e8eb99d",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "This is where we define the model architecture, particularly the rotary positional encoding and the decoder-only transformer, as well as the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57518a1d-1b84-4acb-b191-00211af887fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T11:40:48.277896Z",
     "iopub.status.busy": "2024-08-07T11:40:48.277729Z",
     "iopub.status.idle": "2024-08-07T11:40:48.478758Z",
     "shell.execute_reply": "2024-08-07T11:40:48.478188Z",
     "shell.execute_reply.started": "2024-08-07T11:40:48.277880Z"
    }
   },
   "outputs": [],
   "source": [
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(RotaryPositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        self.register_buffer('sin_pos', torch.sin(position * div_term))\n",
    "        self.register_buffer('cos_pos', torch.cos(position * div_term))\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x1 = x[..., ::2]\n",
    "        x2 = x[..., 1::2]\n",
    "        x = torch.cat([\n",
    "            x1 * self.cos_pos[:seq_len] - x2 * self.sin_pos[:seq_len],\n",
    "            x1 * self.sin_pos[:seq_len] + x2 * self.cos_pos[:seq_len]\n",
    "        ], dim=-1)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super(DecoderOnlyTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = RotaryPositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=4*d_model, dropout=dropout, activation='gelu', batch_first=True, norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, mask=src_mask)\n",
    "        output = self.fc_out(output)\n",
    "        return self.dropout(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2186a203-acea-4489-9586-af2a568ae7f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T11:40:48.479526Z",
     "iopub.status.busy": "2024-08-07T11:40:48.479348Z",
     "iopub.status.idle": "2024-08-07T11:40:50.864289Z",
     "shell.execute_reply": "2024-08-07T11:40:50.863063Z",
     "shell.execute_reply.started": "2024-08-07T11:40:48.479504Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = sp.get_piece_size()  # Use the tokenizer to get the vocabulary size\n",
    "d_model = 256\n",
    "nhead = 4\n",
    "num_encoder_layers = 4\n",
    "dropout = 0.25\n",
    "\n",
    "model = DecoderOnlyTransformer(vocab_size, d_model, nhead, num_layers=num_encoder_layers, dropout=dropout)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7711c9-6267-4636-9af4-c1a0d85f570c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd0c9c61-ca0b-4cf9-8f8f-f598aeb046b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T22:38:29.859382Z",
     "iopub.status.busy": "2024-08-06T22:38:29.858873Z",
     "iopub.status.idle": "2024-08-07T01:45:44.808152Z",
     "shell.execute_reply": "2024-08-07T01:45:44.807712Z",
     "shell.execute_reply.started": "2024-08-06T22:38:29.859360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.8720, Val Loss: 0.4792\n",
      "Saved the best model with val loss: 0.4792\n",
      "Epoch 2, Train Loss: 1.6791, Val Loss: 0.4437\n",
      "Saved the best model with val loss: 0.4437\n",
      "Epoch 3, Train Loss: 1.6650, Val Loss: 0.4346\n",
      "Saved the best model with val loss: 0.4346\n",
      "Epoch 4, Train Loss: 1.6549, Val Loss: 0.4238\n",
      "Saved the best model with val loss: 0.4238\n",
      "Epoch 5, Train Loss: 1.6470, Val Loss: 0.4134\n",
      "Saved the best model with val loss: 0.4134\n",
      "Epoch 6, Train Loss: 1.6414, Val Loss: 0.4077\n",
      "Saved the best model with val loss: 0.4077\n",
      "Epoch 7, Train Loss: 1.6354, Val Loss: 0.4007\n",
      "Saved the best model with val loss: 0.4007\n",
      "Epoch 8, Train Loss: 1.6304, Val Loss: 0.3952\n",
      "Saved the best model with val loss: 0.3952\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "num_epochs = 8\n",
    "batch_size = 64\n",
    "clip_value = 1.0  # gradient clipping\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# OneCycleLR Scheduler setup\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3,  # Peak learning rate\n",
    "    total_steps=total_steps,\n",
    "    anneal_strategy='linear',\n",
    "    final_div_factor=10,  # Final learning rate will be max_lr/div_factor\n",
    "    pct_start=0.3  # Percentage of the cycle to spend increasing the learning rate\n",
    ")\n",
    "\n",
    "def create_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, scheduler, sp):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        src = batch[:, :-1]\n",
    "        tgt = batch[:, 1:]\n",
    "        src_mask = create_mask(src.size(1)).to(device)\n",
    "        output = model(src, src_mask)\n",
    "        loss = criterion(output.contiguous().view(-1, sp.get_piece_size()), tgt.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, criterion, sp):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            src = batch[:, :-1]\n",
    "            tgt = batch[:, 1:]\n",
    "            src_mask = create_mask(src.size(1)).to(device)\n",
    "            output = model(src, src_mask)\n",
    "            loss = criterion(output.contiguous().view(-1, sp.get_piece_size()), tgt.contiguous().view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = 'best_model_.pth'\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, scheduler, sp)\n",
    "    val_loss = validate(model, val_dataloader, criterion, sp)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "        }, best_model_path)\n",
    "        print(f'Saved the best model with val loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a1b3814-9021-4aac-84e8-7e5ecdc7491d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T01:45:44.809331Z",
     "iopub.status.busy": "2024-08-07T01:45:44.809054Z",
     "iopub.status.idle": "2024-08-07T01:45:45.792394Z",
     "shell.execute_reply": "2024-08-07T01:45:45.791922Z",
     "shell.execute_reply.started": "2024-08-07T01:45:44.809331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25e45cc3-ec64-41cd-81a3-63ee5b2835c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T01:45:45.793390Z",
     "iopub.status.busy": "2024-08-07T01:45:45.793046Z",
     "iopub.status.idle": "2024-08-07T01:45:45.797644Z",
     "shell.execute_reply": "2024-08-07T01:45:45.797091Z",
     "shell.execute_reply.started": "2024-08-07T01:45:45.793375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,672,040 trainable parameters\n",
      "The model has 3,672,040 total parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return trainable_params, total_params\n",
    "\n",
    "trainable, total = count_parameters(model)\n",
    "print(f\"The model has {trainable:,} trainable parameters\")\n",
    "print(f\"The model has {total:,} total parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26baf69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,952,040 parameters in total (using state_dict)\n"
     ]
    }
   ],
   "source": [
    "# Count the number of parameters using state_dict, to include all parameters (trainable and non-trainable) and buffers\n",
    "state_dict = model.state_dict()\n",
    "total_params_state_dict = sum(v.numel() for v in state_dict.values())\n",
    "print(f\"The model has {total_params_state_dict:,} parameters in total (using state_dict)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71275114-2b27-48f4-854b-2eb9a8b7b478",
   "metadata": {},
   "source": [
    "## Load and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2d410c7-905f-49bb-b2c9-6a0fc7fb1799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T11:40:50.866349Z",
     "iopub.status.busy": "2024-08-07T11:40:50.865965Z",
     "iopub.status.idle": "2024-08-07T11:40:53.419078Z",
     "shell.execute_reply": "2024-08-07T11:40:53.418541Z",
     "shell.execute_reply.started": "2024-08-07T11:40:50.866310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated molecule: COc1cc2cc(OC)c(OC)cc2cc1C(c1ccccc1)c1ccccc1\n"
     ]
    }
   ],
   "source": [
    "# Load the best model checkpoint\n",
    "checkpoint = torch.load('best_model_mid.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "def create_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "def generate_molecule(model, start_seq, sp_model, max_length=150, temperature=0.7):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        current_seq = start_seq.to(device).unsqueeze(0)  # Add batch dimension\n",
    "        for _ in range(max_length):\n",
    "            src_mask = create_mask(current_seq.size(1)).to(device)\n",
    "            output = model(current_seq, src_mask)\n",
    "            logits = output[0, -1, :] / temperature  # Select last time step\n",
    "            next_token_idx = torch.multinomial(torch.softmax(logits, dim=-1), 1).item()\n",
    "            \n",
    "            if next_token_idx == sp_model.piece_to_id('<EOS>'):\n",
    "                break\n",
    "\n",
    "            next_token_tensor = torch.tensor([[next_token_idx]], device=device)\n",
    "            current_seq = torch.cat([current_seq, next_token_tensor], dim=1)\n",
    "    \n",
    "    # Decode using the tokenizer\n",
    "    generated_sequence = sp_model.decode_ids(current_seq[0].cpu().tolist())\n",
    "    return generated_sequence.replace('<SOS>', '', 1)\n",
    "\n",
    "# Example: Generate a molecule starting with a carbon atom\n",
    "start_seq = torch.tensor([sp.piece_to_id('<SOS>'), sp.piece_to_id('C')], device=device)  # Start with <SOS> and a carbon atom\n",
    "generated_molecule = generate_molecule(model, start_seq, sp)\n",
    "print(\"Generated molecule:\", generated_molecule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62a11263-7c04-46ad-9cf3-af47b6a76827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T01:45:46.131063Z",
     "iopub.status.busy": "2024-08-07T01:45:46.130654Z",
     "iopub.status.idle": "2024-08-07T01:47:31.565360Z",
     "shell.execute_reply": "2024-08-07T01:47:31.564767Z",
     "shell.execute_reply.started": "2024-08-07T01:45:46.131040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3968\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "batch_size = 64\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            src = batch[:, :-1]\n",
    "            tgt = batch[:, 1:]\n",
    "            src_mask = create_mask(src.size(1)).to(device)\n",
    "            output = model(src, src_mask)\n",
    "            loss = criterion(output.contiguous().view(-1, sp.get_piece_size()), tgt.contiguous().view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "test_loss = evaluate(model, test_dataloader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3afd55ff-43cb-493a-a2a4-11d7b4b988a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T11:40:53.420215Z",
     "iopub.status.busy": "2024-08-07T11:40:53.419984Z",
     "iopub.status.idle": "2024-08-07T11:57:10.829277Z",
     "shell.execute_reply": "2024-08-07T11:57:10.828828Z",
     "shell.execute_reply.started": "2024-08-07T11:40:53.420215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated molecule 1000: C(CC)C(=O)Nc1cc(C(=O)NCc2ccccc2)n(C)n1\n",
      "Generated molecule 2000: C(CO)NC(=O)c1cccc(S(=O)(=O)N2CCCC2)c1\n",
      "Generated molecule 3000: COc1ccc(C(=O)Nc2cccnc2N2CCCC2)cc1OCC\n",
      "Generated molecule 4000: Cn1nnc(C(=O)Nc2ccc(OCC(N)=O)cc2)c1C1CC1\n",
      "Generated molecule 5000: Cn1c(SCC(=O)O)nnc1-c1cccs1\n",
      "Generated molecule 6000: COc1ccc(C(=O)Nc2ccccc2O)cc1Cl\n",
      "Generated molecule 7000: Cn1c(SCC(=O)Nc2cccc(OC)c2)nnc1-c1ccc(C)cc1\n",
      "Generated molecule 8000: C(NC(=O)COCc1ccccc1)c1ccc(F)cc1\n",
      "Generated molecule 9000: C(=O)N1CCn2c(nnc2-c2ccc(C)cc2)C1\n",
      "Generated molecule 10000: C1CCCN(C(=O)CCCNC(=O)c2cccnc2OC)C1\n"
     ]
    }
   ],
   "source": [
    "# Generate a large set of molecules (e.g., 10,000 or 30,000)\n",
    "generated_smiles = []\n",
    "for i in range(10000):\n",
    "    start_seq = torch.tensor([sp.piece_to_id('<SOS>'), sp.piece_to_id('C')], device=device)  # Start with <SOS> and a carbon atom\n",
    "    generated_molecule = generate_molecule(model, start_seq, sp)\n",
    "    generated_molecule = generated_molecule  # Remove start of sequence token\n",
    "    generated_smiles.append(generated_molecule)\n",
    "    \n",
    "    # Print every 1000th molecule\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"Generated molecule {i + 1}: {generated_molecule}\")\n",
    "        \n",
    "# Get the training and test set SMILES\n",
    "train_smiles = train_dataset.processed_smiles\n",
    "test_smiles = test_dataset.processed_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7719d1c5-539b-40ad-891a-dfec4bc46d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T11:57:10.830718Z",
     "iopub.status.busy": "2024-08-07T11:57:10.830535Z",
     "iopub.status.idle": "2024-08-07T11:58:45.155338Z",
     "shell.execute_reply": "2024-08-07T11:58:45.154714Z",
     "shell.execute_reply.started": "2024-08-07T11:57:10.830704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validity: 0.95\n",
      "Uniqueness: 0.95\n",
      "Diversity: 0.87\n"
     ]
    }
   ],
   "source": [
    "from tdc import Evaluator\n",
    "\n",
    "# Validity\n",
    "validity_evaluator = Evaluator(name='Validity')\n",
    "validity_score = validity_evaluator(generated_smiles)\n",
    "print(f\"Validity: {validity_score:.2f}\")\n",
    "\n",
    "# Uniqueness\n",
    "uniqueness_evaluator = Evaluator(name='Uniqueness')\n",
    "uniqueness_score = uniqueness_evaluator(generated_smiles)\n",
    "print(f\"Uniqueness: {uniqueness_score:.2f}\")\n",
    "\n",
    "# Diversity\n",
    "diversity_evaluator = Evaluator(name='Diversity')\n",
    "diversity_score = diversity_evaluator(generated_smiles)\n",
    "print(f\"Diversity: {diversity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b894a9-8e4f-43f7-8220-45bd5ae7e453",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T11:58:45.156297Z",
     "iopub.status.busy": "2024-08-07T11:58:45.156131Z",
     "iopub.status.idle": "2024-08-07T12:21:31.406727Z",
     "shell.execute_reply": "2024-08-07T12:21:31.405774Z",
     "shell.execute_reply.started": "2024-08-07T11:58:45.156283Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:07:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[12:07:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[12:08:00] WARNING: not removing hydrogen atom without neighbors\n",
      "[12:08:00] WARNING: not removing hydrogen atom without neighbors\n",
      "[12:08:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[12:08:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[12:09:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[12:09:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[12:09:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[12:09:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[12:09:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[12:09:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[12:10:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[12:10:17] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty: 0.9253\n",
      "KL Divergence: 5.8839\n",
      "Fragment Similarity: 0.0767\n",
      "Scaffold Diversity: 0.6607\n"
     ]
    }
   ],
   "source": [
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter\n",
    "\n",
    "# Convert SMILES to RDKit molecules\n",
    "def smiles_to_mol(smiles_list):\n",
    "    return [Chem.MolFromSmiles(smiles) for smiles in smiles_list if Chem.MolFromSmiles(smiles)]\n",
    "\n",
    "generated_mols = smiles_to_mol(generated_smiles)\n",
    "train_mols = smiles_to_mol(train_smiles)\n",
    "\n",
    "# Novelty\n",
    "def calculate_novelty(generated_mols, train_mols):\n",
    "    train_smiles_set = set(Chem.MolToSmiles(mol) for mol in train_mols)\n",
    "    novel_count = sum(1 for mol in generated_mols if Chem.MolToSmiles(mol) not in train_smiles_set)\n",
    "    return novel_count / len(generated_mols)\n",
    "\n",
    "novelty = calculate_novelty(generated_mols, train_mols)\n",
    "print(f\"Novelty: {novelty:.4f}\")\n",
    "\n",
    "# KL Divergence (using molecular weight as an example property)\n",
    "def calculate_kl_divergence(generated_mols, train_mols):\n",
    "    def get_mol_weights(mols):\n",
    "        return [Chem.Descriptors.ExactMolWt(mol) for mol in mols]\n",
    "    \n",
    "    gen_weights = get_mol_weights(generated_mols)\n",
    "    train_weights = get_mol_weights(train_mols)\n",
    "    \n",
    "    # Create histograms\n",
    "    hist_gen, _ = np.histogram(gen_weights, bins=50, density=True)\n",
    "    hist_train, _ = np.histogram(train_weights, bins=50, density=True)\n",
    "    \n",
    "    # Add small epsilon to avoid division by zero\n",
    "    hist_gen = hist_gen + 1e-10\n",
    "    hist_train = hist_train + 1e-10\n",
    "    \n",
    "    return entropy(hist_gen, hist_train)\n",
    "\n",
    "kl_divergence = calculate_kl_divergence(generated_mols, train_mols)\n",
    "print(f\"KL Divergence: {kl_divergence:.4f}\")\n",
    "\n",
    "# Fragment Similarity\n",
    "def calculate_fragment_similarity(generated_mols, train_mols):\n",
    "    def get_fragments(mol):\n",
    "        return set(Chem.MolToSmiles(frag) for frag in Chem.GetMolFrags(mol, asMols=True))\n",
    "    \n",
    "    train_fragments = set.union(*[get_fragments(mol) for mol in train_mols])\n",
    "    \n",
    "    similarities = []\n",
    "    for mol in generated_mols:\n",
    "        gen_fragments = get_fragments(mol)\n",
    "        if gen_fragments:\n",
    "            similarity = len(gen_fragments.intersection(train_fragments)) / len(gen_fragments)\n",
    "            similarities.append(similarity)\n",
    "    \n",
    "    return np.mean(similarities)\n",
    "\n",
    "fragment_similarity = calculate_fragment_similarity(generated_mols, train_mols)\n",
    "print(f\"Fragment Similarity: {fragment_similarity:.4f}\")\n",
    "\n",
    "# Scaffold Diversity\n",
    "def calculate_scaffold_diversity(mols):\n",
    "    scaffolds = [Chem.MolToSmiles(MurckoScaffold.GetScaffoldForMol(mol)) for mol in mols]\n",
    "    scaffold_counts = Counter(scaffolds)\n",
    "    return len(scaffold_counts) / len(mols)\n",
    "\n",
    "scaffold_diversity = calculate_scaffold_diversity(generated_mols)\n",
    "print(f\"Scaffold Diversity: {scaffold_diversity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
